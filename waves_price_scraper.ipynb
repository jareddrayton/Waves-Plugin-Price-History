{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "The company Waves produces plugins for use in audio production. To remain competitive in an increasingly saturated market it has transitioned to a pricing model where discounts are offered year round, however, the amount of discount applied to each product varies throught out the year by quite a considerable margin. It would be beneficial to have the ability to see how a current sale price compares historically in order to know if it is an optiimum time to buy.\n",
    "\n",
    "The task is to write a scraper to perodically retrieve the pricing for each item of both regular pricing and the sale price. Similar to how camel camel camel tracks prices on Amazon. Allowing a decision to be made when the best time is to buy a product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Target Page\n",
    "\n",
    "The main products page is found at the following URL https://www.waves.com/plugins, however, after visiting this URL, a URL fragment is added and it take the following form\n",
    "\n",
    "https://www.waves.com/plugins#sort:path~type~order=.hidden-price~number~asc|views:view=grid-view|paging:currentPage=0|paging:number=20\n",
    "\n",
    "By default 20 items are displayed, with the need to cycle through multiple pages to see futher items. However, by changing the number \"20\" to \"all\" in the URL fragment, it will display all availiable products on one page. This obviously bypasses the need to introduce any functionality to drive the website. \n",
    "\n",
    "Also, by scanning the list of products we can see that at the very bottom, certain items do not have a regular price or sale price and are only availiable through the purchase of particular bundles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Download the HTML page using requests\n",
    "url = 'https://www.waves.com/plugins#sort:path~type~order=.hidden-price~number~asc|views:view=grid-view|paging:currentPage=0|paging:number=all'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "\n",
    "with open('Waves_plugins.html', 'wb') as webpage:\n",
    "    webpage.write(r.content)  \n",
    "\n",
    "# Open the manually downloaded webpage and create a new BeautifulSoup object\n",
    "with open('Waves_plugins.html', encoding='utf8') as html_file:\n",
    "    soup = BeautifulSoup(html_file, 'lxml')\n",
    "\n",
    "# Remove the HTML file\n",
    "os.remove(\"Waves_plugins.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting the HTML file manually, it looks like data for each product is contained within `<article>` tags.\n",
    "Using `find_all` with the article tag and calling len() on this object shows that there are 203 items with the `<article>` tag, which is the same number of products listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n"
     ]
    }
   ],
   "source": [
    "print(len(soup.find_all('article')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            date       product_id  \\\n",
      "0     2019-11-12 23:05:02.863031    node-ABRDTGMC   \n",
      "1     2019-11-12 23:05:02.865025  node-API2500TDM   \n",
      "2     2019-11-12 23:05:02.868008     node-BSSLAPP   \n",
      "3     2019-11-12 23:05:02.870007     node-BRERMOT   \n",
      "4     2019-11-12 23:05:02.872018      node-BSSDPR   \n",
      "...                          ...              ...   \n",
      "1218  2019-11-12 23:13:21.847148     node-S360TDM   \n",
      "1219  2019-11-12 23:13:21.849162      node-DTSNDM   \n",
      "1220  2019-11-12 23:13:21.851155      node-DTSNUM   \n",
      "1221  2019-11-12 23:13:21.853152     node-DTSNMST   \n",
      "1222  2019-11-12 23:13:21.855150      node-DDSPCH   \n",
      "\n",
      "                       product_title regular_price sale_price  \n",
      "0      Abbey Road TG Mastering Chain           199      29.99  \n",
      "1                           API 2500           299      29.99  \n",
      "2                       Bass Slapper            69      29.99  \n",
      "3                      Brauer Motion            99      29.99  \n",
      "4                        BSS DPR-402           149      29.99  \n",
      "...                              ...           ...        ...  \n",
      "1218   S360 Surround Imager & Panner           NaN        NaN  \n",
      "1219  DTS Neuralâ¢ Surround DownMix           NaN        NaN  \n",
      "1220    DTS Neuralâ¢ Surround UpMix           NaN        NaN  \n",
      "1221       DTS Neuralâ¢ Mono2Stereo           NaN        NaN  \n",
      "1222                    Dugan Speech           NaN        NaN  \n",
      "\n",
      "[1223 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "with open('waves_price_history.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        spamwriter = csv.writer(csvfile, delimiter=' ', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        spamwriter.writerow([\"date\", \"product_id\", \"product_title\", \"regular_price\", \"sale_price\"])\n",
    "\n",
    "# Loop over  \n",
    "for product in soup.find_all('article'):\n",
    "    \n",
    "    product_id = product['id']\n",
    "    product_title = product.find('p', class_='title').get_text(strip=True)\n",
    "    \n",
    "    regular_price = product.find('div', class_='regular-price align-center')\n",
    "\n",
    "    if regular_price != None:\n",
    "        regular_price = regular_price.get_text(strip=True).strip()\n",
    "        regular_price = [s for s in regular_price if s.isnumeric() == True]\n",
    "        regular_price = \"\".join(regular_price) \n",
    "    \n",
    "    sale_price = product.find('div', class_='on-sale-price align-center')\n",
    "    \n",
    "    if sale_price != None:\n",
    "        sale_price = sale_price.get_text(strip=True).strip()\n",
    "        sale_price = [s for s in sale_price if s.isnumeric() == True]\n",
    "        sale_price = \"\".join(sale_price)\n",
    "        sale_price = sale_price[:-2] + \".99\"\n",
    "    \n",
    "    # print(product_id, product_title, regular_price, sale_price)    \n",
    "    \n",
    "    with open('waves_price_history.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        spamwriter = csv.writer(csvfile, delimiter=' ', quotechar='|', quoting=csv.QUOTE_ALL)\n",
    "        spamwriter.writerow([datetime.now(), product_id, product_title, regular_price, sale_price])\n",
    "\n",
    "price_history = pd.read_csv(\"waves_price_history.csv\", encoding = \"ISO-8859-1\", sep=\" \", quotechar='|')        \n",
    "print(price_history)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "1. Move code into a seperate .py file and set up as a cron job that runs periodically.\n",
    "2. Create some kind of interface or way to access the data in a meaningful way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Issues and Bugs\n",
    "\n",
    "1. For products which cost more than 99.99 they have whole number dollar amounts. This breaks the price formatting.\n",
    "2. When updating the csv file, it would be more efficient to only add new data when there is a price change.\n",
    "3. Seems to be some Unicode issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources Used\n",
    "\n",
    "https://www.youtube.com/watch?v=ng2o98k983k\n",
    "    \n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/#\n",
    "    \n",
    "https://stackoverflow.com/questions/30750843/python-3-unicodedecodeerror-charmap-codec-cant-decode-byte-0x9d\n",
    "\n",
    "https://pythonprogramming.net/introduction-scraping-parsing-beautiful-soup-tutorial/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
